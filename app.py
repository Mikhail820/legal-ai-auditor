import streamlit as st
import requests
import re
import base64
import random
from PyPDF2 import PdfReader
from docx import Document
from bs4 import BeautifulSoup

# --- 1. –ù–ê–°–¢–†–û–ô–ö–ò –ò –ë–ï–ó–û–ü–ê–°–ù–û–°–¢–¨ ---
st.set_page_config(page_title="LegalAI Analyzer", layout="wide", page_icon="üõ°Ô∏è")

def anonymize_text(text):
    """–°–∫—Ä—ã–≤–∞–µ—Ç —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (–§–ó-152)"""
    patterns = {
        r'\b\d{4}\s\d{6}\b': '[–ü–ê–°–ü–û–†–¢]',
        r'\b\+?\d{1,3}[-.\s]?\(?\d{1,4}?\)?[-.\s]?\d{1,4}[-.\s]?\d{1,4}\b': '[–¢–ï–õ–ï–§–û–ù]',
        r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b': '[EMAIL]',
    }
    for pattern, replacement in patterns.items():
        text = re.sub(pattern, replacement, text)
    return text

# --- 2. –õ–û–ì–ò–ö–ê API (–ü–£–õ –ö–õ–Æ–ß–ï–ô) ---
def get_api_key():
    # –ò—â–µ—Ç –∫–ª—é—á–∏ GOOGLE_API_KEY_1, _2, _3 –≤ Secrets
    keys = [st.secrets.get(f"GOOGLE_API_KEY_{i}") for i in range(1, 4)]
    valid_keys = [k for k in keys if k]
    return random.choice(valid_keys) if valid_keys else st.secrets.get("GOOGLE_API_KEY")

def call_gemini(prompt, content, is_img=False):
    api_key = get_api_key()
    url = f"https://generativelanguage.googleapis.com/v1/models/gemini-1.5-flash:generateContent?key={api_key}"
    
    if not is_img:
        content = anonymize_text(content)
    
    parts = [{"text": f"{prompt}\n\nCONTENT:\n{content}"}]
    if is_img:
        parts = [{"text": prompt}, {"inline_data": {"mime_type": "image/jpeg", "data": base64.b64encode(content).decode()}}]
    
    payload = {"contents": [{"parts": parts}], "generationConfig": {"temperature": 0.1, "maxOutputTokens": 2000}}
    try:
        r = requests.post(url, json=payload, timeout=30).json()
        return r['candidates'][0]['content']['parts'][0]['text']
    except: return "‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–≤—è–∑–∏ —Å –ò–ò. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –¥—Ä—É–≥–æ–π –º–µ—Ç–æ–¥ –≤–≤–æ–¥–∞ –∏–ª–∏ –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–ª—é—á–∏."

# --- 3. –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –¢–ï–ö–°–¢–ê (–§–ê–ô–õ–´ –ò URL) ---
def extract_from_file(file):
    if file.name.endswith(".pdf"):
        return " ".join([p.extract_text() for p in PdfReader(file).pages])
    elif file.name.endswith(".docx"):
        return "\n".join([p.text for p in Document(file).paragraphs])
    return ""

def extract_from_url(url):
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        r = requests.get(url, headers=headers, timeout=10)
        r.raise_for_status()
        soup = BeautifulSoup(r.text, 'html.parser')
        for s in soup(["script", "style", "nav", "header", "footer"]): s.decompose()
        return soup.get_text(separator=' ')[:25000] # –õ–∏–º–∏—Ç –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
    except Exception as e:
        return f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ URL: {str(e)}"

# --- 4. –ò–ù–¢–ï–†–§–ï–ô–° ---
with st.sidebar:
    st.title("‚öñÔ∏è LegalAI Pro")
    audience = st.radio("–ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∫–∞–∫:", ["–ì—Ä–∞–∂–¥–∞–Ω–∏–Ω", "–ü—Ä–µ–¥–ø—Ä–∏–Ω–∏–º–∞—Ç–µ–ª—å", "–Æ—Ä–∏—Å—Ç"])
    st.divider()
    st.info("üí° –ò–ò –ø–æ–¥—Å—Ç—Ä–æ–∏—Ç—Å—è –ø–æ–¥ –≤–∞—à —É—Ä–æ–≤–µ–Ω—å –∑–Ω–∞–Ω–∏–π –∏ —Ü–µ–ª–∏.")

st.header("–ü—Ä–æ–≤–µ—Ä–∫–∞ —é—Ä–∏–¥–∏—á–µ—Å–∫–æ–π –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏")

# –í–∫–ª–∞–¥–∫–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –≤–≤–æ–¥–∞
tab1, tab2 = st.tabs(["üìÑ –§–∞–π–ª –∏–ª–∏ –§–æ—Ç–æ", "üîó –°—Å—ã–ª–∫–∞ –Ω–∞ –æ—Ñ–µ—Ä—Ç—É"])

with tab1:
    up = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ –¥–æ–≥–æ–≤–æ—Ä", type=["pdf", "docx", "jpg", "png"])

with tab2:
    url_input = st.text_input("–í—Å—Ç–∞–≤—å—Ç–µ —Å—Å—ã–ª–∫—É –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É —Å –¥–æ–≥–æ–≤–æ—Ä–æ–º")

if st.button("üöÄ –ù–ê–ß–ê–¢–¨ –ê–£–î–ò–¢", type="primary"):
    txt_to_analyze = ""
    is_image = False
    
    if up:
        if up.type.startswith("image"):
            txt_to_analyze = up.getvalue()
            is_image = True
        else:
            txt_to_analyze = extract_from_file(up)
    elif url_input:
        with st.spinner("–ß–∏—Ç–∞—é —Å–∞–π—Ç..."):
            txt_to_analyze = extract_from_url(url_input)

    if txt_to_analyze:
        with st.spinner("–ò–ò –∏–∑—É—á–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç..."):
            prompts = {
                "–ì—Ä–∞–∂–¥–∞–Ω–∏–Ω": "–ù–∞–π–¥–∏ –ª–æ–≤—É—à–∫–∏, —Å–∫—Ä—ã—Ç—ã–µ –ø–ª–∞—Ç–µ–∂–∏ –∏ –æ–±—ä—è—Å–Ω–∏ –≤—Å—ë –ø—Ä–æ—Å—Ç—ã–º–∏ —Å–ª–æ–≤–∞–º–∏. Score 0-100.",
                "–ü—Ä–µ–¥–ø—Ä–∏–Ω–∏–º–∞—Ç–µ–ª—å": "–û—Ü–µ–Ω–∏ —Ä–∏—Å–∫–∏ —É–±—ã—Ç–∫–æ–≤, —Å—Ä–æ–∫–∏, —à—Ç—Ä–∞—Ñ—ã –∏ —É—Å–ª–æ–≤–∏—è –≤—ã—Ö–æ–¥–∞. Score 0-100.",
                "–Æ—Ä–∏—Å—Ç": "–ü—Ä–æ–≤–µ—Ä—å –Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∑–∞–∫–æ–Ω–∞–º –†–§, –Ω–∞–π–¥–∏ –ª–∞–∑–µ–π–∫–∏ –∏ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏. Score 0-100."
            }
            main_p = f"Role: Senior Lawyer. Audience: {audience}. {prompts[audience]} Format: SCORE: X/100, ### üî¥ –†–ò–°–ö–ò, ### üü° –°–û–í–ï–¢–´, ### üü¢ –ß–ï–ö-–õ–ò–°–¢."
            
            st.session_state.res = call_gemini(main_p, txt_to_analyze, is_img=is_image)
    else:
        st.error("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø—Ä–µ–¥–æ—Å—Ç–∞–≤—å—Ç–µ —Ñ–∞–π–ª –∏–ª–∏ —Å—Å—ã–ª–∫—É.")

# --- 5. –í–´–í–û–î –†–ï–ó–£–õ–¨–¢–ê–¢–û–í ---
if "res" in st.session_state:
    res = st.session_state.res
    col_res, col_check = st.columns([2, 1])
    
    with col_res:
        st.markdown(res)
    
    with col_check:
        st.subheader("‚úÖ –ß—Ç–æ –Ω—É–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å:")
        steps = re.findall(r"-\s*(.*?)(?:\n|$)", res)
        for i, step in enumerate(steps[:8]):
            st.checkbox(step.strip(), key=f"step_{i}")
