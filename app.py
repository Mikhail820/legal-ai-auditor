import streamlit as st
import requests
import re
import base64
from PyPDF2 import PdfReader
from docx import Document
from bs4 import BeautifulSoup

# --- 1. –ù–ê–°–¢–†–û–ô–ö–ò ---
st.set_page_config(page_title="LegalAI Pro 2.0", layout="wide", page_icon="‚öñÔ∏è")

def reset_app():
    for key in st.session_state.keys():
        del st.session_state[key]
    st.rerun()

def anonymize_text(text):
    patterns = {
        r'\b\d{4}\s\d{6}\b': '[–ü–ê–°–ü–û–†–¢]',
        r'\b\+?\d{1,3}[-.\s]?\(?\d{1,4}?\)?[-.\s]?\d{1,4}[-.\s]?\d{1,4}\b': '[–¢–ï–õ–ï–§–û–ù]',
        r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b': '[EMAIL]',
    }
    for pattern, replacement in patterns.items():
        text = re.sub(pattern, replacement, text)
    return text

# --- 2. API LOGIC (–ü–†–Ø–ú–û–ô POST –ó–ê–ü–†–û–°) ---
def call_gemini(prompt, content, is_img=False):
    # –¢–æ–ª—å–∫–æ –æ–¥–∏–Ω –∫–ª—é—á, –±–µ–∑ –ø—É–ª–∞
    try:
        api_key = st.secrets["GOOGLE_API_KEY"]
    except:
        return "‚ùå –û—à–∏–±–∫–∞: –ö–ª—é—á 'GOOGLE_API_KEY' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ Secrets."

    # –ü—Ä—è–º–æ–π URL –∫ Gemini 2.0 Flash
    url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key={api_key}"
    
    if not is_img:
        content = anonymize_text(content)
    
    parts = [{"text": f"{prompt}\n\nCONTENT:\n{content}"}]
    if is_img:
        parts = [{"text": prompt}, {"inline_data": {"mime_type": "image/jpeg", "data": base64.b64encode(content).decode()}}]
    
    payload = {
        "contents": [{"parts": parts}],
        "generationConfig": {"temperature": 0.1, "maxOutputTokens": 4000}
    }
    
    try:
        response = requests.post(url, json=payload, timeout=30)
        res_json = response.json()
        if "candidates" in res_json:
            return res_json['candidates'][0]['content']['parts'][0]['text']
        else:
            return f"‚ö†Ô∏è –û—à–∏–±–∫–∞ API: {res_json.get('error', {}).get('message', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞')}"
    except Exception as e:
        return f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–≤—è–∑–∏: {str(e)}"

# --- 3. –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –¢–ï–ö–°–¢–ê ---
def extract_text(file):
    try:
        if file.name.endswith(".pdf"):
            return " ".join([p.extract_text() for p in PdfReader(file).pages])
        elif file.name.endswith(".docx"):
            return "\n".join([p.text for p in Document(file).paragraphs])
    except: return "–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞."
    return ""

def extract_from_url(url):
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        r = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(r.text, 'html.parser')
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–π –º—É—Å–æ—Ä —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        for s in soup(["script", "style", "nav", "header", "footer"]): s.decompose()
        return soup.get_text(separator=' ')[:30000]
    except: return "–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ URL."

# --- 4. –ë–û–ö–û–í–ê–Ø –ü–ê–ù–ï–õ–¨ ---
with st.sidebar:
    st.title("‚öñÔ∏è LegalAI Pro 2.0")
    audience = st.radio("–†–æ–ª—å –∞–Ω–∞–ª–∏–∑–∞:", ["–ì—Ä–∞–∂–¥–∞–Ω–∏–Ω", "–ü—Ä–µ–¥–ø—Ä–∏–Ω–∏–º–∞—Ç–µ–ª—å", "–Æ—Ä–∏—Å—Ç"])
    jurisdiction = st.selectbox("–Æ—Ä–∏—Å–¥–∏–∫—Ü–∏—è:", ["–†–§ (–ì–ö, –ö–æ–ê–ü)", "–°–ù–ì", "–ú–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω–æ–µ –ø—Ä–∞–≤–æ"])
    
    st.divider()
    if st.button("üîÑ –°–±—Ä–æ—Å–∏—Ç—å –≤—Å—ë", use_container_width=True):
        reset_app()
    
    st.divider()
    st.success("ü§ñ –ú–æ–¥–µ–ª—å: 2.0 Flash (No SDK)")

# --- 5. –ì–õ–ê–í–ù–´–ô –≠–ö–†–ê–ù ---
tab1, tab2 = st.tabs(["üìÑ –î–æ–∫—É–º–µ–Ω—Ç / –§–æ—Ç–æ", "üîó –°—Å—ã–ª–∫–∞"])

with tab1:
    up = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ –¥–æ–≥–æ–≤–æ—Ä", type=["pdf", "docx", "jpg", "png"])

with tab2:
    url_input = st.text_input("–í—Å—Ç–∞–≤—å—Ç–µ —Å—Å—ã–ª–∫—É –Ω–∞ –æ—Ñ–µ—Ä—Ç—É")

if st.button("üöÄ –ù–ê–ß–ê–¢–¨ –ê–£–î–ò–¢", type="primary", use_container_width=True):
    txt_to_analyze = ""
    is_image = False
    
    if up:
        if up.type.startswith("image"):
            txt_to_analyze, is_image = up.getvalue(), True
        else:
            txt_to_analyze = extract_text(up)
    elif url_input:
        with st.spinner("–ß–∏—Ç–∞—é —Å–∞–π—Ç..."):
            txt_to_analyze = extract_from_url(url_input)

    if txt_to_analyze:
        with st.spinner(f"–ê–Ω–∞–ª–∏–∑–∏—Ä—É—é –¥–ª—è: {audience}..."):
            prompts = {
                "–ì—Ä–∞–∂–¥–∞–Ω–∏–Ω": "–ù–∞–π–¥–∏ —Å–∫—Ä—ã—Ç—ã–µ —Ä–∏—Å–∫–∏ –∏ —à—Ç—Ä–∞—Ñ—ã. –ü–∏—à–∏ –ø—Ä–æ—Å—Ç–æ.",
                "–ü—Ä–µ–¥–ø—Ä–∏–Ω–∏–º–∞—Ç–µ–ª—å": "–§–æ–∫—É—Å –Ω–∞ —Å—Ä–æ–∫–∏, –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å –∏ —à—Ç—Ä–∞—Ñ—ã. Score 0-100.",
                "–Æ—Ä–∏—Å—Ç": f"–ê–Ω–∞–ª–∏–∑ –ø–æ –ø—Ä–∞–≤—É {jurisdiction}. –ü–æ–∏—Å–∫ –∫–æ–ª–ª–∏–∑–∏–π –∏ –ª–∞–∑–µ–µ–∫."
            }
            full_p = f"Role: Senior Lawyer. Audience: {audience}. Jurisdiction: {jurisdiction}. {prompts[audience]} Format: SCORE: X/100, ### üî¥ –†–ò–°–ö–ò, ### üü° –°–û–í–ï–¢–´, ### üü¢ –ü–õ–ê–ù –î–ï–ô–°–¢–í–ò–ô (—Å–ø–∏—Å–∫–æ–º)."
            
            st.session_state.res = call_gemini(full_p, txt_to_analyze, is_img=is_image)
    else:
        st.error("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞.")

# --- 6. –í–´–í–û–î ---
if "res" in st.session_state:
    res = st.session_state.res
    left, right = st.columns([2, 1])
    
    with left:
        st.subheader("üìä –ê–Ω–∞–ª–∏–∑")
        sections = res.split("###")
        for s in sections:
            if "üî¥" in s: st.error(s)
            elif "üü°" in s: st.warning(s)
            elif "üü¢" in s: st.success(s)
            else: st.markdown(s)

    with right:
        st.subheader("‚úÖ –ß–µ–∫-–ª–∏—Å—Ç")
        steps = re.findall(r"-\s*(.*?)(?:\n|$)", res)
        for i, step in enumerate(steps[:10]):
            st.checkbox(step.strip(), key=f"ch_{i}")
